{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging HGDP and 1000 Genomes\n",
    "\n",
    "This script will walk you through the merging of the HGDP and 1000 Genomes genotypes.\n",
    "We will follow these steps:\n",
    "- Download HGDP files, and transform them into a plink file\n",
    "- Download 1000G files and keep only SNPs found in the HGDP files\n",
    "- Merge the HGDP and 1000G files\n",
    "\n",
    "## Needed packages\n",
    "\n",
    "To run this script you'll need [plink](https://www.cog-genomics.org/plink2), [vcftools](https://vcftools.github.io/index.html), [bcftools](https://samtools.github.io/bcftools/bcftools.html).\n",
    "All of these can be found in [bioconda](https://bioconda.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "First let's import modules and set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, shutil, subprocess, csv, time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tomas/Documents/HGDP_1000G_Merge\n"
     ]
    }
   ],
   "source": [
    "projpath = os.path.realpath(\"..\")\n", ##I had to add the actual folder to this direction
    "pathhgdp = os.path.join(projpath, \"DataBases\", \"HGDP\")\n",
    "path1000 = os.path.join(projpath, \"DataBases\", \"1000G\")\n",
    "print(projpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HGDP\n",
    "\n",
    "The HGDP files (Stanford) were downloaded from [here](http://hagsc.org/hgdp/files.html), and the sample list file from [here](http://www.stanford.edu/group/rosenberglab/data/rosenberg2006ahg/SampleInformation.txt).\n",
    "The script to transform the HGDP data to plink format is called HGDPtoPlink.sh and was modified from [here](http://www.harappadna.org/2011/02/hgdp-to-ped-conversion/).\n",
    "The HGDP data uses coordinates from build 36.1 (a list of assemblies can be found [here](https://genome.ucsc.edu/FAQ/FAQreleases.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['bash', 'HGDPtoPlink.sh'], returncode=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run the HGDPtoPlink script\n",
    "#It can take a while\n",
    "os.chdir(os.path.join(projpath, \"Code\"))\n",
    "subprocess.run([\"bash\", \"HGDPtoPlink.sh\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the 1000G uses the GRCh37 assembly (fasta file can be found [here](http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/) as human_g1k_v37.fasta.gz) we'll need to liftover the HGDP coordinates.\n",
    "To do that we'll use UCSC [liftOver](http://genome.ucsc.edu/cgi-bin/hgLiftOver) already installed using bioconda, and [liftOverPlink](https://github.com/sritchie73/liftOverPlink) as a wrapper to work with plink files (`ped` and `map` formats).\n",
    "The chain file that tells liftOver how to convert between hg18 and hg19 can be downloaded [here](http://hgdownload.cse.ucsc.edu/goldenPath/hg18/liftOver/hg18ToHg19.over.chain.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting MAP file to UCSC BED file...\n",
      "SUCC:  map->bed succ\n",
      "Lifting BED file...\n",
      "SUCC:  liftBed succ\n",
      "Converting lifted BED file back to MAP...\n",
      "SUCC:  bed->map succ\n",
      "cleaning up BED files...\n"
     ]
    }
   ],
   "source": [
    "os.chdir(pathhgdp)\n",
    "#Using liftover\n",
    "liftoverplink = os.path.join(projpath, \"Code\", \"liftOverPlink\")\n",
    "%run $liftoverplink --map hgdp940.map --out lifted --chain hg18ToHg19.over.chain.gz ##Didn't run from python. Use console\n",
    "badlifts = os.path.join(projpath, \"Code\", \"rmBadLifts\")\n",
    "%run $badlifts --map lifted.map --out good_lifted.map --log bad_lifted.dat\n",
    "#Creating a list of snps to include in lifted version\n",
    "snps = pd.read_csv(\"good_lifted.map\", sep = \"\\t\", header = None)\n",
    "snps.iloc[:,1].to_csv(\"snplist.txt\", index = False)\n",
    "#Excluding snps and creating binary file\n",
    "subprocess.run([\"plink\", \"--file\", \"hgdp940\", \"--recode\", \"--out\", \"lifted\", \"--extract\", \"snplist.txt\" ]) ##Didn't run from python. Use console\n",
    "subprocess.run([\"plink\", \"--ped\", \"lifted.ped\", \"--map\", \"good_lifted.map\", \"--make-bed\", \"--out\", \"hgdp940hg19\"])##Didn't run from python. Use console\n",
    "\n",
    "#Removing some files\n",
    "for file in glob.glob(\"*.ped\"):\n",
    "    os.remove(file)\n",
    "    \n",
    "for file in glob.glob(\"*.map\"):\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing single-pass .bed write (644054 variants, 940 people).\n",
      "644054 variants loaded from .bim file.\n",
      "644054 variants and 940 people pass filters and QC.\n",
      "Finished file... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read hgdp940hg19 log file\n",
    "with open(\"hgdp940hg19.log\") as myfile:\n",
    "    for num, line in enumerate(myfile, 1):\n",
    "        if \"variants\" in line:\n",
    "            print(line, end='')\n",
    "    print(\"Finished file... \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_dir = os.path.join(projpath, \"Results\")\n",
    "for filename in glob.glob(\"hgdp940hg19.*\"):\n",
    "    shutil.copy(filename, dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1000G\n",
    "\n",
    "The 1000G Phase 3 files were downloaded from [here](ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/).\n",
    "First, we will extract the list of snps (`snplist.txt`) from the HGDP dataset for each chromosome of the 1000G samples using vcftools.\n",
    "Then we will concatenate the different autosomal chromosomes in one file and convert it into a plink binary file using bcftools and plink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting snps found in the HGDP to the 1000G files\n",
    "#Takes time\n",
    "os.chdir(path1000)\n",
    "shutil.copy(os.path.join(pathhgdp, \"snplist.txt\"), path1000)\n",
    "for file in glob.glob(\"*chr[0-9]*.gz\"):\n",
    "    outname = file.split(\".\")[1] + \"_extracted\"\n",
    "    subprocess.run([\"vcftools\", \"--gzvcf\", file, \"--snps\", \"snplist.txt\", \"--recode\", \"--out\", outname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['bcftools', 'concat', '-o', '1000g.vcf.gz', '-Oz', 'chr21_extracted.recode.vcf', 'chr13_extracted.recode.vcf', 'chr4_extracted.recode.vcf', 'chr19_extracted.recode.vcf', 'chr8_extracted.recode.vcf', 'chr18_extracted.recode.vcf', 'chr14_extracted.recode.vcf', 'chr3_extracted.recode.vcf', 'chr11_extracted.recode.vcf', 'chr7_extracted.recode.vcf', 'chr16_extracted.recode.vcf', 'chr10_extracted.recode.vcf', 'chr17_extracted.recode.vcf', 'chr5_extracted.recode.vcf', 'chr2_extracted.recode.vcf', 'chr6_extracted.recode.vcf', 'chr9_extracted.recode.vcf', 'chr1_extracted.recode.vcf', 'chr15_extracted.recode.vcf', 'chr12_extracted.recode.vcf', 'chr22_extracted.recode.vcf', 'chr20_extracted.recode.vcf'], returncode=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatfiles = glob.glob(\"chr[0-9]*.recode.vcf\")\n",
    "function = [\"bcftools\", \"concat\", \"-o\", \"1000g.vcf.gz\", \"-Oz\"]\n",
    "function.extend(concatfiles)\n",
    "subprocess.run(function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to binary plink\n",
    "subprocess.run([\"plink\", \"--vcf\", \"1000g.vcf.gz\", \"--make-bed\", \"--out\", \"1000Ghg19\" ])\n",
    "#Updating fam file\n",
    "allfam = pd.read_csv(\"integrated_call_samples_v2.20130502.ALL.ped\", header = None, skiprows = 1, sep = \"\\t\")\n",
    "oldfam = pd.read_csv(\"1000Ghg19.fam\", header = None, sep = \" \")\n",
    "updatedfam = pd.merge(oldfam, allfam, how = \"inner\", left_on = 1, right_on = 1)\n",
    "updatedfam.iloc[:,[6,1,7,8,9,5]].to_csv(\"1000Ghg19.fam\", sep = \" \", header = False, index = False)\n",
    "os.remove(\"1000g.vcf.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob(\"chr*.recode.vcf\"):\n",
    "    os.remove(file)\n",
    "    \n",
    "dest_dir = os.path.join(projpath, \"Results\")\n",
    "for filename in glob.glob(\"1000Ghg19.*\"):\n",
    "    shutil.copy(filename, dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge reference samples\n",
    "\n",
    "Now we will merge the 1000G and HGDP databases, both using the hg19 coordinates and with related people removed.\n",
    "We will attempt an initial merge, and then flip strands for the problematic SNPs. \n",
    "Then we will try a second attempt, and remove the still problematic snps from both databases, to then attempt the final merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLINK v1.90b4 64-bit (20 Mar 2017)\n",
      "Options in effect:\n",
      "  --bfile hgdp940hg19_temp\n",
      "  --bmerge 1000Ghg19_flip_temp\n",
      "  --make-bed\n",
      "  --out hgdp1000ghg19\n",
      "\n",
      "Hostname: tomasgazelle\n",
      "Working directory: /home/tomas/Documents/HGDP_1000G_Merge/Results\n",
      "Start time: Thu May 17 09:36:00 2018\n",
      "\n",
      "Random number seed: 1526564160\n",
      "3865 MB RAM detected; reserving 1932 MB for main workspace.\n",
      "940 people loaded from hgdp940hg19_temp.fam.\n",
      "2504 people to be merged from 1000Ghg19_flip_temp.fam.\n",
      "Of these, 2504 are new, while 0 are present in the base dataset.\n",
      "Warning: Multiple positions seen for variant 'rs9460309'.\n",
      "Warning: Multiple positions seen for variant 'rs11967812'.\n",
      "Warning: Multiple positions seen for variant 'rs11975477'.\n",
      "Warning: Multiple positions seen for variant 'rs13233990'.\n",
      "Warning: Multiple positions seen for variant 'rs11771665'.\n",
      "Warning: Multiple positions seen for variant 'rs2249255'.\n",
      "Warning: Multiple positions seen for variant 'rs4935071'.\n",
      "Warning: Multiple positions seen for variant 'rs9507310'.\n",
      "Warning: Multiple positions seen for variant 'rs1475276'.\n",
      "Warning: Multiple positions seen for variant 'rs12433837'.\n",
      "Warning: Multiple positions seen for variant 'rs1017238'.\n",
      "Warning: Multiple positions seen for variant 'rs2362394'.\n",
      "Warning: Multiple positions seen for variant 'rs1495901'.\n",
      "Warning: Multiple positions seen for variant 'rs9624480'.\n",
      "644003 markers loaded from hgdp940hg19_temp.bim.\n",
      "640456 markers to be merged from 1000Ghg19_flip_temp.bim.\n",
      "Of these, 0 are new, while 640456 are present in the base dataset.\n",
      "Performing single-pass merge (3444 people, 644003 variants).\n",
      "Merged fileset written to hgdp1000ghg19-merge.bed + hgdp1000ghg19-merge.bim +\n",
      "hgdp1000ghg19-merge.fam .\n",
      "644003 variants loaded from .bim file.\n",
      "3444 people (1851 males, 1593 females) loaded from .fam.\n",
      "Using 1 thread (no multithreaded calculations invoked).\n",
      "Before main variant filters, 3432 founders and 12 nonfounders present.\n",
      "Calculating allele frequencies... done.\n",
      "Total genotyping rate is 0.995654.\n",
      "644003 variants and 3444 people pass filters and QC.\n",
      "Note: No phenotypes present.\n",
      "--make-bed to hgdp1000ghg19.bed + hgdp1000ghg19.bim + hgdp1000ghg19.fam ...\n",
      "done.\n",
      "\n",
      "End time: Thu May 17 09:36:32 2018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.chdir(os.path.join(projpath, \"Results\"))\n",
    "#Preliminary merge\n",
    "subprocess.run([\"plink\", \"--bfile\", \"1000Ghg19\", \"--bmerge\", \"hgdp940hg19\", \"--make-bed\", \"--out\", \"hgdp1000ghg19\"])\n",
    "#Flip snps\n",
    "subprocess.run([\"plink\", \"--bfile\", \"1000Ghg19\", \"--flip\", \"hgdp1000ghg19-merge.missnp\", \"--make-bed\", \"--out\", \"1000Ghg19_flip\"])\n",
    "subprocess.run([\"plink\", \"--bfile\", \"1000Ghg19_flip\", \"--bmerge\", \"hgdp940hg19\", \"--make-bed\", \"--out\", \"hgdp1000ghg19\"])\n",
    "#Removing snps\n",
    "for file in glob.glob(\"*.bed\"):\n",
    "    outname = file.split(\".\")[0] + \"_temp\"\n",
    "    subprocess.run([\"plink\", \"--bfile\", file.split(\".\")[0], \"--exclude\", \"hgdp1000ghg19-merge.missnp\", \"--make-bed\", \"--out\", outname])\n",
    "\n",
    "#Removing temp files\n",
    "subprocess.run([\"plink\", \"--bfile\", \"hgdp940hg19_temp\", \"--bmerge\", \"1000Ghg19_flip_temp\", \"--make-bed\", \"--out\", \"hgdp1000ghg19\"])\n",
    "for file in glob.glob(\"*_temp*\"):\n",
    "    os.remove(file)\n",
    "    \n",
    "for file in glob.glob(\"*_flip*\"):\n",
    "    os.remove(file)\n",
    "\n",
    "with open(\"hgdp1000ghg19.log\", 'r') as fin:\n",
    "    file_contents = fin.read()\n",
    "    print(file_contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
